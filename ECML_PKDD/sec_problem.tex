\section{Methodology}

\subsection{Conditional average treatment effects}

Suppose we have a data set with n independently and identically distributed (iid) units with $ i = 1, \cdots, n$, for each unit, it has a feature vector $X_{i}  \in [0,1]^d$, a response $Y_{i} \in \mathbb{R}$ and treatment indicator $W_{i} \in \{0,1\}$.\\
For a unit-level causal effect, we can use the Rubin causal model and consider the treatment effect on unit $i$ being
$\tau_{i} = Y_{i}(1) - Y_{i}(0)$. 
%to estimate the average causal effect as %shown in function \ref{eq:1},
%\begin{equation} \label{eq:1}
%\tau_{i} = Y_{i}(1) - Y_{i}(0) \\
%\end{equation}
In this paper, we are interested in calculating the heterogenous causal effect, which we define as
$\tau(x) = \mathbb{E} \big[{Y_{i}(1) - Y_{i}(0) \mid X_{i} = x}\big]$ following \cite{RePEc:ecm:emetrp:v:71:y:2003:i:4:p:1161-1189}.
%as \ref{eq:2}, this estimator is proposed by \cite{RePEc:ecm:emetrp:v:71:y:2003:i:4:p:1161-1189},
%\begin{equation} \label{eq:2}
%\tau(x) = \mathbb{E} \big[{Y_{i}(1) - Y_{i}(0) \mid X_{i} = x}\big]\\
%\end{equation}
Of course, in an observational study, a unit is either untreated or not, so we know either $Y_{i}(0)$ or $Y_{i}(1)$ but not both. However,  one can still estimate $\tau(x)$ if one assumes unconfoundedness:
\begin{equation} \label{eq:3}
W_{i} \perp \left( Y_{i}(1), Y_{i}(0) \right)  \mid X_{i}\\
\end{equation}
Under the unconfoundedness assumption, Athey and Imbens \cite{Athey} show that one can estimate the causal effect as 

\begin{equation}\label{eq:3.5}
\tau(x) = \mathbb{E} \big[   Y^{*} \mid X_{i} =  x\big] 
\end{equation}
where the transformed outcome $Y^{*}$  is defined as
\begin{equation} \label{eq:4}
%Y_{i}^{*} =  Y_{i}^{obs} \cdot \frac{W_{i} - e(X_{i})}{e(X_{i}) \cdot (1 - e(X_{i}))} \\
Y_{i}^{*} =  Y_{i} \cdot \frac{W_{i} - e(X_{i})}{e(X_{i}) \cdot (1 - e(X_{i}))} 
\end{equation}
%
% is function \ref{eq:4}, 
and the propensity score function $e(x)$ is defined as $e(x)= \mathbb{E} \big[  W_{i} \mid X_{i} =  x \big]$.
%Function $e(x)$ is function \ref{eq:4.5}, to estimate the propensity score, 
%
Several approaches to estimate the propensity score are known \cite{rose:rubi:cent:1983}, \cite{HoImaKin07} including logic regression which we decided to use in this paper.
%%
%
%\begin{equation}\label{eq:4.5}
%e(x) = \mathbb{E} \big[  W_{i} \mid X_{i} =  x \big]
%\end{equation}

\subsection{Transformed Outcome Tree}
The transformed outcomes tree is a regression tree with $Y^{*}$ instead of $Y$. As mentioned above, the transformed outcome is calculated with  (\ref{eq:4}), then we can use traditional regression tree method to estimate the causal effect as 
$\tau(x) = \mathbb{E} \big[   Y^{*} \mid X_{i} =  x\big] $ based on the average transformed outcome of all nodes in the leaf of the tree where $i$ resides in.

\subsection{Causal Tree Model}
We use regression tree to estimate the heterogeneous causal effects, the first step is to construct the tree. To construct the regression tree, we recursively partition the node until the size of the node is less than a threshold we set or the gain of split is negative.\\
In classic regression tree, mean square error (MSE) is often used to as the criterion for node splitting, the average value within the node is used as the estimator. Following Asthey and Imbens \cite{1504.01132}, we use (\ref{eq:5}) as the estimator and we calculate the error of the node by summing $Y_{i}^ {*}- \hat{\tau}(X_{i})$.
\begin{equation} \label{eq:5}
%\begin{split}
\hat{\tau}^{CT}(X_{i}) = \sum_{i:X_{i} \in \mathbb{X}_{l}}  Y_{i}^{obs} \cdot \frac{ W_{i} / \hat{e}(X_{i})  }{\sum_{i:X_{i} \in \mathbb{X}_{l}}  W_{i} / \hat{e}(X_{i})}  \\
 - \sum_{i:X_{i} \in \mathbb{X}_{l}}  Y_{i}^{obs} \cdot\frac{ (1 - W_{i}) / (1 - \hat{e}(X_{i}))  }{\sum_{i:X_{i} \in \mathbb{X}_{l}}  (1 - W_{i}) / (1 - \hat{e}(X_{i}))}
% \end{split}
\end{equation}

\subsection{Pruning the tree}

To avoid overfitting of the tree, we need to prune the tree. We use the minimal cost complexity pruning and we define it as \ref{eq:6}. $\alpha$ is the complexity parameter, with it we can construct the regression with the right size.
\begin{equation}\label{eq:6}
R_{\alpha}(T) = R(T) + \alpha \left|\tilde{T}\right|
\end{equation}
where R(T) is the resubstitution error estimate of tree T, $\left|\tilde{T}\right|$ is defined as the complexity of the tree, which is the number of leaves in the tree,\\
To estimate the error of a node, we use function \ref{eq:7},
\begin{equation}\label{eq:7}
R(t) = \sum_{i=1}^{N}(Y_{i} - \hat{\tau}(X_{i}))
\end{equation}
where N is the total units in the nodes. \\

To get a sequence of $\alpha$, we minimize function \ref{eq:8},
\begin{equation}\label{eq:8}
g(t,T) = \frac{R(t) - R(T_{t})}{\left|\tilde{T_{t}}\right| - 1}
\end{equation}
where $T_{t}$ is a subtree of T rooted at node t.\\
We use the weakest link cutting to determine $\alpha$ and use it as the complexity parameter when we build the tree for the whole data set. \\
We use the train data set to construct the tree and then apply weakest link cutting to the tree with $\alpha$ starting with 0. Until there is one node in the tree, we get a series of
$\alpha$, $ \alpha_{0} < \alpha_{1} < \cdots < \alpha_{k}$.
Then we set $\beta_{0} $ = 0, $\beta_{1} = \sqrt{\alpha_{1}\alpha_{2}}$, $\cdots $, $\beta_{k-1} = \sqrt{\alpha_{k-1}\alpha_{know}}$

We use V-fold cross validation to estimate the errors for different $\beta$ and use the $\beta$ with minimum error.
We divide the data into k sets randomly with the same size, we use $1,2,3,\cdots,v-1,k$ to represent the v-th data part and (k) as the left part of the data correspond to the v-th part. For each $\beta_{k}$, we useV-fold cross validation to get the estimated error. The error is calculated by function \ref{eq:10},
 \begin{equation}\label{eq:10}
Err(\beta;Y_{i}^{v}, X_{i}^{v}) =  \sum_{1}^{N}(Y_{i}^{v} - \hat{\tau}(X_{i}^{(v)}))
\end{equation}
where $N = n/V$, n is the size of the whole data set. 
We can then calculate error $R(T(\beta)) $for each $\beta$ value as the average error:
\begin{equation}\label{eq:9}
R(T(\beta)) = \frac{1}{V}\sum_{1}^{V}Err(\beta;Y_{i}^{v}, X_{i}^{v})
\end{equation}

\subsection{Random forest with TOT trees}











